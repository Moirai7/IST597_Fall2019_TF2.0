# -*- coding: utf-8 -*-
"""ist597_lstm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pT9a5BBeycmzFOvn761E5Cw0gPECCaJH
"""

# -*- coding: utf-8 -*-
"""IST597_LSTM.ipynb
# IST597
Routine to create RNN Cells in Tensorflow 2.0 using eager execution.
# Code adapted from Google AI Language Team
"""

import os
import numpy as np
import tensorflow as tf
import time
import matplotlib.pyplot as plt
tf.random.set_seed(seed=2612)
np.random.seed(2612)

num_classes = 10
np.random.seed(133)

class BasicLSTM(tf.keras.Model):
    def __init__(self, units, return_sequence=False, return_states=False, **kwargs):
        super(BasicLSTM, self).__init__(**kwargs)
        self.units = units
        self.return_sequence = return_sequence
        self.return_states = return_states

        def bias_initializer(_, *args, **kwargs):
            # Unit forget bias from the paper
            # - [Learning to forget: Continual prediction with LSTM](http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)
            return tf.keras.backend.concatenate([
                tf.keras.initializers.Zeros()((self.units,), *args, **kwargs),  # input gate
                tf.keras.initializers.Ones()((self.units,), *args, **kwargs),  # forget gate
                tf.keras.initializers.Zeros()((self.units * 2,), *args, **kwargs),  # context and output gates
            ])

        self.kernel = tf.keras.layers.Dense(4 * units, use_bias=False)
        self.recurrent_kernel = tf.keras.layers.Dense(4 * units, kernel_initializer='glorot_uniform', bias_initializer=bias_initializer)

    def call(self, inputs, training=None, mask=None, initial_states=None):
        # LSTM Cell in pure TF Eager code
        # reset the states initially if not provided, else use those
        if initial_states is None:
            h_state = tf.zeros((inputs.shape[0], self.units))
            c_state = tf.zeros((inputs.shape[0], self.units))
        else:
            assert len(initial_states) == 2, "Must pass a list of 2 states when passing 'initial_states'"
            h_state, c_state = initial_states

        h_list = []
        c_list = []
        for t in range(inputs.shape[1]):
            # LSTM gate steps
            ip = inputs[:, t, :]
            z = self.kernel(ip)
            z += self.recurrent_kernel(h_state)

            z0 = z[:, :self.units]
            z1 = z[:, self.units: 2 * self.units]
            z2 = z[:, 2 * self.units: 3 * self.units]
            z3 = z[:, 3 * self.units:]

            # gate updates
            i = tf.keras.activations.sigmoid(z0)
            f = tf.keras.activations.sigmoid(z1)
            c = f * c_state + i * tf.nn.tanh(z2)

            # state updates
            o = tf.keras.activations.sigmoid(z3)
            h = o * tf.nn.tanh(c)

            h_state = h
            c_state = c

            h_list.append(h_state)
            c_list.append(c_state)

        hidden_outputs = tf.stack(h_list, axis=1)
        hidden_states = tf.stack(c_list, axis=1)

        if self.return_states and self.return_sequence:
            return hidden_outputs, [hidden_outputs, hidden_states]
        elif self.return_states and not self.return_sequence:
            return hidden_outputs[:, -1, :], [h_state, c_state]
        elif self.return_sequence and not self.return_states:
            return hidden_outputs
        else:
            return hidden_outputs[:, -1, :]

class BasicGRU(tf.keras.Model):
    def __init__(self, units, return_sequence=False, return_states=False, **kwargs):
        super(BasicGRU, self).__init__(**kwargs)
        self.units = units
        self.return_sequence = return_sequence
        self.return_states = return_states

        def bias_initializer(_, *args, **kwargs):
            # Unit forget bias from the paper
            # - [Learning to forget: Continual prediction with LSTM](http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)
            return tf.keras.backend.concatenate([
                tf.keras.initializers.Zeros()((self.units,), *args, **kwargs),  # reset gate
                tf.keras.initializers.Ones()((self.units,), *args, **kwargs),  # update gate
            ])

        self.kernel = tf.keras.layers.Dense(2 * units, use_bias=False)
        self.recurrent_kernel = tf.keras.layers.Dense(2 * units, kernel_initializer='glorot_uniform', bias_initializer=bias_initializer)

        self.unique_kernel = tf.keras.layers.Dense(units, use_bias=False)
        self.unique_recurrent_kernel = tf.keras.layers.Dense(units, kernel_initializer='glorot_uniform', bias_initializer='zeros')
        #self.output_recurrent_kernel = tf.keras.layers.Dense(units, kernel_initializer='glorot_uniform', bias_initializer='zeros')

    def call(self, inputs, training=None, mask=None, initial_states=None):
        # GRU Cell in pure TF Eager code
        # reset the states initially if not provided, else use those
        if initial_states is None:
            h_state = tf.zeros((inputs.shape[0], self.units))
            #c_state = tf.zeros((inputs.shape[0], self.units))
        else:
            assert len(initial_states) == 1, "Must pass a list of 1 states when passing 'initial_states'"
            h_state = initial_states

        h_list = []
        o_list = []
        for t in range(inputs.shape[1]):
            # GRU gate steps
            ip = inputs[:, t, :]
            z = self.kernel(ip)
            z += self.recurrent_kernel(h_state)

            z0 = z[:, :self.units]
            z1 = z[:, self.units: 2 * self.units]

            # gate updates
            r_t = tf.keras.activations.sigmoid(z0)
            z_t = tf.keras.activations.sigmoid(z1)

            # candidate activation
            h_t = tf.nn.tanh(self.unique_kernel(ip) + self.unique_recurrent_kernel(h_state * r_t))

            # state updates
            h = (tf.ones_like(z_t)  - z_t) * h_state + z_t * h_t
            #o = tf.nn.relu(self.output_recurrent_kernel(h))
            o = h

            h_state = h
            o_state = o

            h_list.append(h_state)
            o_list.append(o_state)

        hidden_outputs = tf.stack(h_list, axis=1)
        output_outputs = tf.stack(o_list, axis=1)

        if self.return_states and self.return_sequence:
            return hidden_outputs, [hidden_outputs, output_outputs]
        elif self.return_states and not self.return_sequence:
            return hidden_outputs[:, -1, :], [h_state, o_state]
        elif self.return_sequence and not self.return_states:
            return hidden_outputs
        else:
            return hidden_outputs[:, -1, :]

class BasicMGU(tf.keras.Model):
    def __init__(self, units, return_sequence=False, return_states=False, **kwargs):
        super(BasicMGU, self).__init__(**kwargs)
        self.units = units
        self.return_sequence = return_sequence
        self.return_states = return_states

        def bias_initializer(_, *args, **kwargs):
            # Unit forget bias from the paper
            # - [Learning to forget: Continual prediction with LSTM](http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)
            return tf.keras.initializers.Ones()((self.units,), *args, **kwargs)  # update gate

        self.kernel = tf.keras.layers.Dense(units, use_bias=False)
        self.recurrent_kernel = tf.keras.layers.Dense(units, kernel_initializer='glorot_uniform', bias_initializer=bias_initializer)

        self.unique_kernel = tf.keras.layers.Dense(units, use_bias=False)
        self.unique_recurrent_kernel = tf.keras.layers.Dense(units, kernel_initializer='glorot_uniform', bias_initializer='zeros')

    def call(self, inputs, training=None, mask=None, initial_states=None):
        # GRU Cell in pure TF Eager code
        # reset the states initially if not provided, else use those
        if initial_states is None:
            h_state = tf.zeros((inputs.shape[0], self.units))
            #c_state = tf.zeros((inputs.shape[0], self.units))
        else:
            assert len(initial_states) == 1, "Must pass a list of 1 states when passing 'initial_states'"
            h_state = initial_states

        h_list = []
        o_list = []
        for t in range(inputs.shape[1]):
            # GRU gate steps
            ip = inputs[:, t, :]
            z = self.kernel(ip)
            z += self.recurrent_kernel(h_state)

            z0 = z[:, :self.units]

            # gate updates
            u_t = tf.keras.activations.sigmoid(z0)

            # candidate activation
            h_t = tf.nn.tanh(self.unique_kernel(ip) + self.unique_recurrent_kernel(h_state * u_t))

            # state updates
            h = (tf.ones_like(u_t)  - u_t) * h_state + u_t * h_t
            #o = tf.nn.relu(self.output_recurrent_kernel(h))
            o = h

            h_state = h
            o_state = o

            h_list.append(h_state)
            o_list.append(o_state)

        hidden_outputs = tf.stack(h_list, axis=1)
        output_outputs = tf.stack(o_list, axis=1)

        if self.return_states and self.return_sequence:
            return hidden_outputs, [hidden_outputs, output_outputs]
        elif self.return_states and not self.return_sequence:
            return hidden_outputs[:, -1, :], [h_state, o_state]
        elif self.return_sequence and not self.return_states:
            return hidden_outputs
        else:
            return hidden_outputs[:, -1, :]

class Model(tf.keras.Model):
    def __init__(self, types, class_num, units, return_sequence=False, return_states=False, **kwargs):
        super(Model, self).__init__(**kwargs)
        types = types.lower()
        self.layernorm = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.kernel = tf.keras.layers.Dense(class_num, use_bias=False)
        if types == 'lstm':
            self.rnn = BasicLSTM(units, return_sequence, return_states)
        elif types == 'gru':
            self.rnn = BasicGRU(units, return_sequence, return_states)
        elif types == 'mgu':
            self.rnn = BasicMGU(units, return_sequence, return_states)
        else:
            assert False, "lstm, gru, mgu"
        
    def call(self, inputs, training=None, mask=None, initial_states=None):
        logits = self.rnn(inputs, training, mask, initial_states)
        logits = self.layernorm(logits)
        logits = self.kernel(logits)
        return logits


import glob
from matplotlib.image import imread
BASE_PATH = './data/notMNIST_small/'
end = '/*.png'
pixel_depth = 255
data = []
label = []
for idx, fn in enumerate(['A','B','C','D','E','F','G','H','I','J']):
  tmp = []
  for path in glob.glob(BASE_PATH + fn + end):
    try:
      image = (imread(path).astype(float)) / pixel_depth
    except:
      continue
    tmp.append(image)
  label.extend([idx]*len(tmp))
  data.extend(tmp)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.3)

buffer_size = 10000
batch_size = 1000
n_epochs = 3
learning_rate = 0.001
train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))
test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))
train_dataset = train_dataset.shuffle(buffer_size).batch(batch_size,drop_remainder=False)
test_dataset = test_dataset.shuffle(buffer_size).batch(batch_size,drop_remainder=False)

optimizer = tf.optimizers.Adam(learning_rate=learning_rate)

from sklearn.metrics import accuracy_score
def predict(logits):
   return tf.argmax(tf.nn.softmax(logits),1)

def cal_acc(logits, label):
   return accuracy_score(predict(logits), label)

def cal_loss(logits, actual):
    total_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits,labels = actual)
    return total_loss 

def test(mymodel):
   _accs = 0
   logitsall = []
   yall=[]
   for ids,(_x,_y) in test_dataset.enumerate():
      logits = mymodel(_x)
      logitsall.extend(logits)
      yall.extend(_y)
   return cal_acc(logitsall, yall), tf.reduce_sum(cal_loss(logitsall, yall))

def train(types):
    train_loss_results = []
    train_accuracy_results = []
    test_loss_results = []
    test_accuracy_results = []
    mymodel = Model(types, class_num = 10 , units = 128)
    for i in range(n_epochs):
        total_loss = 0
        n_batches = 0
        acc = tf.metrics.Accuracy()
        loss_avg = tf.metrics.Mean()
        start = time.time()
        for idx, (_x, _y) in train_dataset.enumerate():
            with tf.GradientTape() as tape:
                logits = mymodel(_x)
                loss = cal_loss(logits, _y)
            #print(tf.reduce_mean(loss),cal_acc(logits, _y))
            gradients = tape.gradient(loss, mymodel.trainable_variables)
            optimizer.apply_gradients(zip(gradients, mymodel.trainable_variables))
            loss_avg(loss)
            acc(tf.argmax(tf.nn.softmax(logits),1), _y)
        end = time.time()
        print(end-start)
        a,l = test(mymodel)
        test_loss_results.append(l)
        test_accuracy_results.append(a)
        train_loss_results.append(loss_avg.result())
        train_accuracy_results.append(acc.result())
        print("Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}".format(i,loss_avg.result(),acc.result()))
    plot_loss(train_loss_results, types+'_train_loss')
    plot_loss(test_loss_results, types+'_test_loss')
    plot_loss(train_accuracy_results, types+'_train_acc')
    plot_loss(test_accuracy_results, types+'_test_acc')
    print(train_loss_results,test_loss_results,train_accuracy_results,test_accuracy_results)

def plot_loss(loss, name):
   plt.clf()
   plt.plot(range(1,n_epochs+1), loss, 'darkseagreen')
   plt.savefig(name+'.pdf', dpi=600)


train('mgu')

train('lstm')

train('gru')
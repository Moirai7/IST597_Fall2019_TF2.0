# -*- coding: utf-8 -*-
"""forgetting_mlp.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MWWSkFOp5SuzthucCjkF_xOEh-FKPCe9
"""
'''
# Commented out IPython magic to ensure Python compatibility.
try:
  # %tensorflow_version only exists in Colab.
#   %tensorflow_version 2.x
except Exception:
  pass
'''
import os
os.environ['TF_CPP_MIN_LOG_LEVEL']='3'

import numpy as np
import tensorflow as tf
import time
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.metrics import accuracy_score

num_tasks_to_run = 10
num_epochs_per_task = 20

learning_rate = 0.001
batch_size = 10000
image_sz = 28
size_input = image_sz * image_sz
size_hidden = 256
size_output = 10

(taska_train_images, taska_train_labels), (taska_test_images, taska_test_labels) = tf.keras.datasets.mnist.load_data()

'''
#different categories
train_splits = pd.DataFrame(zip(taska_train_labels, taska_train_images.reshape(-1,size_input)))
train_splits[1] = train_splits[1]/255.
train_splits = train_splits.sort_values(by=[0])
test_splits = pd.DataFrame(zip(taska_test_labels, taska_test_images.reshape(-1,size_input)))
test_splits[1] = test_splits[1]/255.
test_splits = test_splits.sort_values(by=[0])
'''

'''

taska_train_images = tf.reshape(tf.cast(taska_train_images, dtype=tf.float32),[-1, size_input])
taska_train_labels = tf.cast(taska_train_labels, dtype=tf.int64)
taska_train_labels /= 255.

taska_test_images = tf.reshape(tf.cast(taska_test_images, dtype=tf.float32),[-1, size_input])
taska_test_labels = tf.cast(taska_test_labels, dtype=tf.int64)
taska_test_labels /= 255.

#new task
(taskb_train_images, taskb_train_labels), (taskb_test_images, taskb_test_labels) = tf.keras.datasets.fashion_mnist.load_data()
taskb_train_images = tf.reshape(tf.cast(taskb_train_images, dtype=tf.float32),[-1, size_input])
taskb_train_labels = tf.cast(taskb_train_labels, dtype=tf.int64)
taskb_train_images /= 255.


taskb_test_images = tf.reshape(tf.cast(taskb_test_images, dtype=tf.float32),[-1, size_input])
taskb_test_labels = tf.cast(taskb_test_labels, dtype=tf.int64)
taskb_test_images /= 255.
taskb_test_ds = tf.data.Dataset.from_tensor_slices((taskb_test_images, taskb_test_labels)).batch(batch_size,drop_remainder=True)
'''

def imgToTensor(img):
  return tf.reshape(tf.cast(img, dtype=tf.float32),[-1, size_input])
  
taska_train_images = taska_train_images/255.
taska_test_images = taska_test_images/255.
taska_train_images = taska_train_images.reshape([-1, size_input])
taska_test_images = taska_test_images.reshape([-1, size_input])

# Generate the tasks specifications as a list of random permutations of the input pixels.
task_permutation = []
for task in range(num_tasks_to_run):
	task_permutation.append(np.random.permutation(size_input))
  
split_train_img = [imgToTensor(taska_train_images)]
split_test_img = [imgToTensor(taska_test_images)]
for k in range(num_tasks_to_run):
  split_train_img.append(imgToTensor(taska_train_images[:,task_permutation[k]]))
  split_test_img.append(imgToTensor(taska_test_images[:,task_permutation[k]]))

taska_train_labels = tf.cast(taska_train_labels, dtype=tf.int32)
taska_test_labels = tf.cast(taska_test_labels, dtype=tf.int32)

import matplotlib as mpl
mpl.rcParams['figure.dpi'] = 600
def plot_images(images, y, yhat=None):
    assert len(images) == len(y) == 9
    
    # Create figure with 3x3 sub-plots.
    fig, axes = plt.subplots(3, 3)
    fig.subplots_adjust(hspace=0.3, wspace=0.3)

    for i, ax in enumerate(axes.flat):
        # Plot image.
        ax.imshow(images[i].reshape(28,28), cmap='binary')

        # Show true and predicted classes.
        if yhat is None:
            xlabel = "True: {0}".format(y[i])
        else:
            xlabel = "True: {0}, Pred: {1}".format(y[i], yhat[i])

        #ax.set_xlabel(xlabel)
        
        # Remove ticks from the plot.
        ax.set_xticks([])
        ax.set_yticks([])
    plt.show()
    
#plot_images(split_train_img[0][0:9].numpy(),[1]*9)
#plot_images(split_train_img[1][0:9].numpy(),[1]*9)

# Define class to build mlp model
loss_func = 1
optm = 1
depth = 3
dropout_prob = 0.
class MLP(tf.Module):
  def __init__(self):
    self.device = 'gpu'
    self._variables = []
    self.W1 = tf.Variable(tf.random.truncated_normal([size_input, size_hidden], stddev=0.05))
    self.b1 = tf.Variable(tf.random.truncated_normal([size_hidden]))
    self._variables.append(self.W1)
    self._variables.append(self.b1)
    for i in range(depth-2):
      W = tf.Variable(tf.random.truncated_normal([size_hidden, size_hidden], stddev=0.05))
      b = tf.Variable(tf.random.truncated_normal([size_hidden]))
      self._variables.append(W)
      self._variables.append(b)
    self.W2 = tf.Variable(tf.random.truncated_normal([size_hidden, size_output], stddev=0.05))
    self.b2 = tf.Variable(tf.random.truncated_normal([size_output]))
    self._variables.append(self.W2)
    self._variables.append(self.b2)
    if optm == 1:
      self.optimizer = tf.optimizers.Adam(learning_rate=learning_rate)
    elif optm == 2:
      self.optimizer = tf.optimizers.SGD(learning_rate=learning_rate, momentum=0.01)
    else:
      self.optimizer = tf.optimizers.RMSprop(learning_rate=learning_rate)
    

  def forward(self, X):
    if self.device is not None:
      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):
        self.y = self.compute_output(X)
    else:
      self.y = self.compute_output(X)

    return self.y

  def loss(self, logits, y_true):
    if loss_func == 1:
      return tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits,labels = y_true)
    elif loss_func == 2:#l1
      return tf.losses.mean_absolute_error(tf.nn.softmax(logits), tf.one_hot(y_true, size_output))
    elif loss_func == 3:#l2
      return tf.losses.MSE(tf.one_hot(y_true, size_output), tf.nn.softmax(logits))
    elif loss_func == 4:#l1+l2
      return tf.losses.mean_absolute_error(tf.one_hot(y_true, size_output), tf.nn.softmax(logits)) + tf.losses.MSE(tf.one_hot(y_true, size_output), tf.nn.softmax(logits))
  
  def pred(self, logits):
    return tf.argmax(tf.nn.softmax(logits),1)

  @tf.function(input_signature=[tf.TensorSpec(shape=[batch_size, size_input], dtype=tf.float32), tf.TensorSpec(shape=[batch_size], dtype=tf.int32),tf.TensorSpec(shape=None,dtype=tf.bool), tf.TensorSpec(shape=None,dtype=tf.float32)])
  def __call__(self, X_train, y_train, train, dropout):
    self.dropout = dropout
    
    with tf.GradientTape() as tape:
      logits = self.forward(X_train)
      current_loss = self.loss(logits, y_train)
      pred = self.pred(logits)
    
    def cal_grad():
      grads = tape.gradient(current_loss, self._variables)
      self.optimizer.apply_gradients(zip(grads, self._variables))
      return logits, current_loss, pred
      
    def cal_not_grad():
      return logits, current_loss, pred
      
    return tf.cond(train, true_fn = cal_grad, false_fn = cal_not_grad)

  def compute_output(self, X):
    what = tf.matmul(X, self.W1) + self.b1
    X = tf.nn.relu(what)
    X = tf.cond(self.dropout != 0., lambda: tf.nn.dropout(X,self.dropout), lambda: X)
    for i in range(1, depth-1):
      # Compute values in hidden layer
      what = tf.matmul(X, self._variables[i*2]) + self._variables[i*2+1]
      X = tf.nn.relu(what)
      X = tf.cond(self.dropout != 0., lambda: tf.nn.dropout(X,self.dropout), lambda: X)
    # Compute output
    output = tf.matmul(X, self.W2) + self.b2
    output = tf.cond(self.dropout != 0., lambda: tf.nn.dropout(output,self.dropout), lambda: output)
    return output

def test(img, lab, path):
    _model = tf.saved_model.load(path)
    acc_avg = tf.metrics.Accuracy()
    loss_avg = tf.metrics.Mean()
    test_ds = tf.data.Dataset.from_tensor_slices((img, lab)).shuffle(1000, seed=2612).batch(batch_size,drop_remainder=True)
    for ids,(_x,_y) in test_ds.enumerate():
      logits,loss,pred = _model(_x, _y, False, dropout = 0.)
      loss_avg(loss)
      acc_avg(pred, _y)
    return loss_avg.result(), acc_avg.result()

def plot_loss(loss, name, epochs):
  plt.clf()
  w_min = np.min(loss)
  w_max = np.max(loss)
  plt.plot(range(1,epochs+1), loss, 'darkseagreen')
  plt.ylim([w_min,w_max])
  plt.show()
  #plt.savefig(name+'.pdf', dpi=600)

# Initialize model using CPU
def train(train_images, train_labels, test_img, test_lab, path, epochs = num_epochs_per_task):
  if tf.saved_model.contains_saved_model(path):
    print('load model:' + path)
    mlp_on_cpu = tf.saved_model.load(path)#MLP()
  else:
    print('create new model')
    mlp_on_cpu = MLP()
  time_start = time.time()
  train_loss_results = []
  train_accuracy_results = []
  test_loss_results = []
  test_accuracy_results = []
  for epoch in range(epochs):
    train_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(1000, seed=epoch*(2612)).batch(batch_size,drop_remainder=True)
    acc_avg = tf.metrics.Accuracy()
    loss_avg = tf.metrics.Mean()
    for inputs, outputs in train_ds:
      logits,loss,pred = mlp_on_cpu(inputs, outputs, True ,dropout_prob)
      loss_avg(loss)
      acc_avg(pred, outputs)
    #tf.saved_model.save(mlp_on_cpu, path)
    #l, a = test(test_img, test_lab, path)
    #print('Number of Epoch = {} - Average MSE:= {:.10f} - ACC:={:.4f}; TEST Loss:={:.10f} - ACC:={:.4f}'.format(
    #    epoch + 1, loss_avg.result() / train_images.shape[0], acc_avg.result(), l, a))
    #test_loss_results.append(l)
    #test_accuracy_results.append(a)
    #train_loss_results.append(loss_avg.result())
    #train_accuracy_results.append(acc_avg.result())
  #plot_loss(train_loss_results, 'imgs/loss' + str(loss_func)+ '_task'+ _task +'_train_loss', epochs)
  #plot_loss(test_loss_results, 'imgs/loss' + str(loss_func)+ '_task'+ _task +'_test_loss', epochs)
  #plot_loss(train_accuracy_results, 'imgs/loss' + str(loss_func)+ '_task'+ _task +'_train_acc', epochs)
  #plot_loss(test_accuracy_results, 'imgs/loss' + str(loss_func)+ '_task'+ _task +'_test_acc', epochs)
  time_taken = time.time() - time_start
  tf.saved_model.save(mlp_on_cpu, path)

  print('\nTotal time taken (in seconds): {:.2f}'.format(time_taken))

'''
#different categories
R = [np.zeros(10)  for i in range(10)]
for k in range(10):
  train_ds = train_splits[train_splits[0]==k]
  images = tf.cast(train_ds[1], dtype = tf.float32)
  labels = tf.cast(train_ds[0], dtype = tf.int64)
  test_ds = test_splits[test_splits[0]==k]
  test_img = tf.cast(test_ds[1], dtype = tf.float32)
  test_lab = tf.cast(test_ds[0], dtype = tf.int64)
  if k == 0:
    train(images, labels, test_img, test_lab, 'models/task2/', 50)
  else:
    train(images, labels, test_img, test_lab, 'models/task2/', 20)
    
  
  _, acc = test(test_img, test_lab,'models/task2/')
  R[k][k] = acc
  if k!=9:
    test_ds = test_splits[test_splits[0]==k+1]
    test_img = tf.cast(test_ds[1], dtype = tf.float32)
    test_lab = tf.cast(test_ds[0], dtype = tf.int64)
    _, acc = test(test_img, test_lab,'models/task2/')
    R[k][k+1] = acc
  print(R)
'''

'''
train_img = []
train_label = []
for k in range(0,60000,6000):
  train_img.append(taska_train_images[k:k+6000])
  train_label.append(taska_train_labels[k:k+6000])
test_img = []
test_label = []  
for k in range(0,10000,1000):
  test_img.append(taska_test_images[k:k+1000])
  test_label.append(taska_test_labels[k:k+1000])
  
R = [np.zeros(num_tasks_to_run)  for i in range(num_tasks_to_run)]
for k in range(num_tasks_to_run):
  if k == 0:
    train(train_img[k], train_label[k], test_img[k], test_label[k], 'models/task0/', 50)
  else:
    train(train_img[k], train_label[k], test_img[k], test_label[k], 'models/task0/', 20)
  _, acc = test(test_img[k], test_label[k],'models/task0/')
  R[k][k] = acc
  if k != 0:
    _, acc = test(test_img[k-1], test_label[k-1],'models/task0/')
    R[k][k-1] = acc
  print(R)
'''

_task = 'lan'
task_name = 'experiment1'
def experiment():
  global _task
  R = [np.zeros(num_tasks_to_run)  for i in range(num_tasks_to_run)]
  for k in range(num_tasks_to_run):
    _task = str(k)
    if k == 0:
      train(split_train_img[k], taska_train_labels, split_test_img[k], taska_test_labels, 'models/'+task_name+'/'+str(loss_func)+'/'+str(optm)+'/'+str(depth)+'/'+str(dropout_prob)+'/', 50)
      _, acc = test(split_test_img[k], taska_test_labels,'models/'+task_name+'/'+str(loss_func)+'/'+str(optm)+'/'+str(depth)+'/'+str(dropout_prob)+'/')
      R[k][k] = acc
    else:
      train(split_train_img[k], taska_train_labels, split_test_img[k], taska_test_labels, 'models/'+task_name+'/'+str(loss_func)+'/'+str(optm)+'/'+str(depth)+'/'+str(dropout_prob)+'/', 20)
      for i in range(k+1):
        _, acc = test(split_test_img[i], taska_test_labels,'models/'+task_name+'/'+str(loss_func)+'/'+str(optm)+'/'+str(depth)+'/'+str(dropout_prob)+'/')
        R[k][i] = acc
    print(R)
    
  G = np.zeros(num_tasks_to_run)
  #G[0] = R[0][0]
  for k in range(0, num_tasks_to_run):
    _task = 'gen'+str(k)
    train(split_train_img[k], taska_train_labels, split_test_img[k], taska_test_labels, 'models/'+task_name+'/'+str(loss_func)+'/'+str(optm)+'/'+str(depth)+'/'+str(dropout_prob)+'/gen'+str(k)+'/', 20)
    _, acc = test(split_test_img[k], taska_test_labels,'models/'+task_name+'/'+str(loss_func)+'/'+str(optm)+'/'+str(depth)+'/'+str(dropout_prob)+'/gen'+str(k)+'/')
    G[k] = acc
    print(G)
  return R, G

def metrics_acc(R):
  _sum = 0
  for i in range(num_tasks_to_run):
    _sum += R[num_tasks_to_run-1][i]
    
  return _sum/num_tasks_to_run

def metrics_bwt(R):
  _sum = 0
  for i in range(num_tasks_to_run-1):
    _sum += (R[num_tasks_to_run-1][i] - R[i][i])
    
  return _sum/(num_tasks_to_run-1)
  
  
def metrics_cbwt(t, R):
  _sum = 0
  for i in range(t+1, num_tasks_to_run):
    _sum += (R[i][t] - R[t][t]) 
    
  return _sum/(num_tasks_to_run-1-t) 
  
  
def metrics_tbwt(R, G):
  _sum = 0
  for i in range(num_tasks_to_run-1):
    _sum += (R[num_tasks_to_run-1][i] - G[i])
  
  return _sum/(num_tasks_to_run-1)

## depth experiments
loss_func = 1
optm = 1
depth = 3
dropout_prob = 0.

for i in [2,3,4]:
  depth = i
  task_name = 'experiment3'
  print('loss: '+ str(loss_func)+'; optm: '+ str(optm)+'; depth: '+str(depth)+'; dropout: '+str(dropout_prob))
  R, G = experiment()
  print(R, G)
  print('acc: {:.10f}, bwt: {:.10f}, tbwt: {:.10f}'.format(metrics_acc(R), metrics_bwt(R), metrics_tbwt(R, G)))
  for t in range(9):
    print('cbwt {}: {:.10f}'.format(t, metrics_cbwt(t, R)))

## dropout_prob experiments
loss_func = 1
optm = 1
depth = 3
dropout_prob = 0.

for i in [.4, .2, .6]:
  dropout_prob = i
  task_name = 'experiment4'
  print('loss: '+ str(loss_func)+'; optm: '+ str(optm)+'; depth: '+str(depth)+'; dropout: '+str(dropout_prob))
  R, G = experiment()
  print(R, G)
  print('acc: {:.10f}, bwt: {:.10f}, tbwt: {:.10f}'.format(metrics_acc(R), metrics_bwt(R), metrics_tbwt(R, G)))
  for t in range(9):
    print('cbwt {}: {:.10f}'.format(t, metrics_cbwt(t, R)))

## Loss func experiments

ss_func = 1
optm = 1
depth = 3
dropout_prob = 0.

for i in [1, 2, 3, 4]:
  loss_func = i
  task_name = 'experiment1'
  print('loss: '+ str(loss_func)+'; optm: '+ str(optm)+'; depth: '+str(depth)+'; dropout: '+str(dropout_prob))
  R, G = experiment()
  print(R, G)
  print('acc: {:.10f}, bwt: {:.10f}, tbwt: {:.10f}'.format(metrics_acc(R), metrics_bwt(R), metrics_tbwt(R, G)))
  for t in range(9):
    print('cbwt {}: {:.10f}'.format(t, metrics_cbwt(t, R)))

## optm experiments
loss_func = 1
optm = 1
depth = 3
dropout_prob = 0.

for i in [1,2,3]:
  optm = i
  task_name = 'experiment2'
  print('loss: '+ str(loss_func)+'; optm: '+ str(optm)+'; depth: '+str(depth)+'; dropout: '+str(dropout_prob))
  R, G = experiment()
  print(R, G)
  print('acc: {:.10f}, bwt: {:.10f}, tbwt: {:.10f}'.format(metrics_acc(R), metrics_bwt(R), metrics_tbwt(R, G)))
  for t in range(9):
    print('cbwt {}: {:.10f}'.format(t, metrics_cbwt(t, R)))


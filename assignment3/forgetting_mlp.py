# -*- coding: utf-8 -*-
"""forgetting_mlp.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MWWSkFOp5SuzthucCjkF_xOEh-FKPCe9
"""

# Commented out IPython magic to ensure Python compatibility.
try:
  # %tensorflow_version only exists in Colab.
#   %tensorflow_version 2.x
except Exception:
  pass

import os
os.environ['TF_CPP_MIN_LOG_LEVEL']='2'

import numpy as np
import tensorflow as tf
import time
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score

(taska_train_images, taska_train_labels), (taska_test_images, taska_test_labels) = tf.keras.datasets.fashion_mnist.load_data()

num_tasks_to_run = 10

num_epochs_per_task = 10

learning_rate = 0.001
batch_size = 100
image_sz = 28
size_input = image_sz * image_sz
size_hidden = 128
size_output = 10

taska_train_images = tf.reshape(tf.cast(taska_train_images, dtype=tf.float32),[-1, size_input])
taska_train_labels = tf.cast(taska_train_labels, dtype=tf.int64)
taska_train_images /= 255.


taska_test_images = tf.reshape(tf.cast(taska_test_images, dtype=tf.float32),[-1, size_input])
taska_test_labels = tf.cast(taska_test_labels, dtype=tf.int64)
taska_test_images /= 255.
taska_test_ds = tf.data.Dataset.from_tensor_slices((taska_test_images, taska_test_labels)).batch(batch_size,drop_remainder=True)

# Define class to build mlp model
class MLP(tf.Module):
  def __init__(self):
    self.device = 'gpu'
    self.W1 = tf.Variable(tf.random.truncated_normal([size_input, size_hidden], stddev=0.05))
    self.b1 = tf.Variable(tf.random.normal([1, size_hidden]))
    self.W2 = tf.Variable(tf.random.truncated_normal([size_hidden, size_output], stddev=0.05))
    self.b2 = tf.Variable(tf.random.normal([1, size_output]))
    self.optimizer = tf.optimizers.Adam(learning_rate=learning_rate)
    self._variables = [self.W1, self.W2, self.b1, self.b2]

  def forward(self, X):
    if self.device is not None:
      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):
        self.y = self.compute_output(X)
    else:
      self.y = self.compute_output(X)

    return self.y

  def loss(self, logits, y_true):
    return tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits,labels = y_true)
  
  def pred(self, logits, y_true):
    return tf.argmax(tf.nn.softmax(logits),1)

  @tf.function(input_signature=[tf.TensorSpec(shape=[batch_size, size_input], dtype=tf.float32), tf.TensorSpec(shape=[batch_size], dtype=tf.int64),tf.TensorSpec(shape=None,dtype=tf.bool), tf.TensorSpec(shape=None,dtype=tf.float32)])
  def __call__(self, X_train, y_train, train, dropout):
    self.dropout = dropout
    
    with tf.GradientTape() as tape:
      logits = self.forward(X_train)
      current_loss = self.loss(logits, y_train)
      pred = self.pred(logits, y_train)
    
    def cal_grad():
      grads = tape.gradient(current_loss, self._variables)
      self.optimizer.apply_gradients(zip(grads, self._variables))
      return logits, current_loss, pred
      
    def cal_not_grad():
      return logits, current_loss, pred
      
    return tf.cond(train, true_fn = cal_grad, false_fn = cal_not_grad)

  def compute_output(self, X):
    # Compute values in hidden layer
    what = tf.matmul(X, self.W1) + self.b1
    hhat = tf.nn.relu(what)
    # Compute output
    output = tf.matmul(hhat, self.W2) + self.b2
    return output

def test():
   _model = tf.saved_model.load('models/')
   acc_avg = tf.metrics.Accuracy()
   loss_avg = tf.metrics.Mean()
   for ids,(_x,_y) in taska_test_ds.enumerate():
      logits,loss,pred = _model(_x, _y, False, dropout = 0.)
      loss_avg(loss)
      acc_avg(pred, _y)
   return loss_avg.result(), acc_avg.result()

def plot_loss(loss, name):
   plt.clf()
   w_min = np.min(loss)
   w_max = np.max(loss)
   print(w_min)
   plt.plot(range(1,num_epochs_per_task+1), loss, 'darkseagreen')
   plt.ylim([w_min,w_max])
   plt.show()
   #plt.savefig(name+'.pdf', dpi=600)

# Initialize model using CPU
def train():
  mlp_on_cpu = MLP()

  time_start = time.time()
  train_loss_results = []
  train_accuracy_results = []
  test_loss_results = []
  test_accuracy_results = []
  for epoch in range(num_epochs_per_task):
    train_ds = tf.data.Dataset.from_tensor_slices((taska_train_images, taska_train_labels)).shuffle(1000, seed=epoch*(2612)).batch(batch_size,drop_remainder=True)
    acc_avg = tf.metrics.Accuracy()
    loss_avg = tf.metrics.Mean()
    for inputs, outputs in train_ds:
      logits,loss,pred = mlp_on_cpu(inputs, outputs, True ,0.)
      loss_avg(loss)
      acc_avg(pred, outputs)
    print('Number of Epoch = {} - Average MSE:= {:.10f} - ACC:={:.4f}'.format(epoch + 1, loss_avg.result() / taska_train_images.shape[0], acc_avg.result()))
    l, a = test()
    test_loss_results.append(l)
    test_accuracy_results.append(a)
    train_loss_results.append(loss_avg.result())
    train_accuracy_results.append(acc_avg.result())
  plot_loss(train_loss_results, 'train_loss')
  plot_loss(test_loss_results, 'test_loss')
  plot_loss(train_accuracy_results, 'train_acc')
  plot_loss(test_accuracy_results, 'test_acc')
  print(train_loss_results,test_loss_results,train_accuracy_results,test_accuracy_results)  
  time_taken = time.time() - time_start

  tf.saved_model.save(mlp_on_cpu, 'models/')

  print('\nTotal time taken (in seconds): {:.2f}'.format(time_taken))

train()

test()